#!/usr/bin/env python3
"""
å›¾ç‰‡ç”ŸæˆæœåŠ¡æ€§èƒ½æµ‹è¯•è„šæœ¬
åŒ…å«åŸºå‡†æµ‹è¯•ã€è´Ÿè½½æµ‹è¯•ã€å¹¶å‘æµ‹è¯•ç­‰
"""

import asyncio
import json
import time
import statistics
import random
import concurrent.futures
from datetime import datetime
from typing import List, Dict, Any, Tuple
import sys
import os
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass, asdict
import boto3
from botocore.exceptions import ClientError

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'lambdas'))

from image_processing_service_optimized import (
    ImageProcessingServiceOptimized,
    ImageRequest,
    ImageResponse
)


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_name: str
    request_id: str
    prompt: str
    model_used: str
    generation_time: float
    from_cache: bool
    success: bool
    error_message: str = ""
    timestamp: datetime = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    total_requests: int
    successful_requests: int
    failed_requests: int
    cache_hits: int
    min_time: float
    max_time: float
    avg_time: float
    median_time: float
    p95_time: float
    p99_time: float
    throughput: float
    error_rate: float
    cache_hit_rate: float


class ImagePerformanceTester:
    """å›¾ç‰‡ç”ŸæˆæœåŠ¡æ€§èƒ½æµ‹è¯•å™¨"""

    def __init__(self, service: ImageProcessingServiceOptimized = None):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.service = service or ImageProcessingServiceOptimized(
            enable_monitoring=True,
            enable_caching=True,
            enable_batching=True
        )
        self.test_results: List[TestResult] = []
        self.test_prompts = self._generate_test_prompts()

    def _generate_test_prompts(self) -> List[str]:
        """ç”Ÿæˆæµ‹è¯•æç¤ºè¯"""
        base_prompts = [
            "ä¸“ä¸šå•†åŠ¡æ¼”ç¤ºèƒŒæ™¯ï¼Œç°ä»£ç§‘æŠ€é£æ ¼ï¼Œè“è‰²ä¸»é¢˜",
            "æ•°æ®åˆ†æä»ªè¡¨æ¿ï¼Œå¯è§†åŒ–å›¾è¡¨ï¼Œæ·±è‰²èƒŒæ™¯",
            "å›¢é˜Ÿåä½œåœºæ™¯ï¼ŒåŠå…¬å®¤ç¯å¢ƒï¼Œæ¸©æš–è‰²è°ƒ",
            "äººå·¥æ™ºèƒ½æ¦‚å¿µå›¾ï¼Œæœªæ¥ç§‘æŠ€ï¼ŒæŠ½è±¡è‰ºæœ¯",
            "äº‘è®¡ç®—æ¶æ„å›¾ï¼ŒæŠ€æœ¯ç¤ºæ„ï¼Œæ¸…æ™°ç»“æ„",
            "å¸‚åœºè¥é”€ç­–ç•¥ï¼Œåˆ›æ„è®¾è®¡ï¼Œé²œè‰³è‰²å½©",
            "é‡‘èæ•°æ®å±•ç¤ºï¼Œä¸“ä¸šå›¾è¡¨ï¼Œç»¿è‰²ä¸Šæ¶¨è¶‹åŠ¿",
            "æ•™è‚²åŸ¹è®­åœºæ™¯ï¼Œäº’åŠ¨å­¦ä¹ ï¼Œæ˜äº®ç¯å¢ƒ",
            "åŒ»ç–—å¥åº·ä¸»é¢˜ï¼Œæ¸…æ´ç®€çº¦ï¼Œè“ç™½é…è‰²",
            "ç”µå­å•†åŠ¡å¹³å°ï¼Œè´­ç‰©ç•Œé¢ï¼Œç°ä»£è®¾è®¡"
        ]

        # ç”Ÿæˆå˜ä½“
        prompts = []
        for base in base_prompts:
            prompts.append(base)
            prompts.append(f"{base}ï¼Œ4Ké«˜æ¸…ï¼Œä¸“ä¸šæ‘„å½±")
            prompts.append(f"{base}ï¼Œæç®€ä¸»ä¹‰ï¼Œæ‰å¹³è®¾è®¡")

        return prompts

    async def test_single_request(self, prompt: str, test_name: str = "single") -> TestResult:
        """æµ‹è¯•å•ä¸ªè¯·æ±‚"""
        request = ImageRequest(
            prompt=prompt,
            request_id=f"test_{test_name}_{int(time.time() * 1000)}",
            priority=5
        )

        start_time = time.time()
        try:
            response = await self.service.generate_image_async(request)
            generation_time = time.time() - start_time

            result = TestResult(
                test_name=test_name,
                request_id=request.request_id,
                prompt=prompt,
                model_used=response.model_used,
                generation_time=generation_time,
                from_cache=response.from_cache,
                success=True
            )

        except Exception as e:
            generation_time = time.time() - start_time
            result = TestResult(
                test_name=test_name,
                request_id=request.request_id,
                prompt=prompt,
                model_used="error",
                generation_time=generation_time,
                from_cache=False,
                success=False,
                error_message=str(e)
            )

        self.test_results.append(result)
        return result

    async def test_concurrent_requests(self, num_requests: int = 10) -> List[TestResult]:
        """æµ‹è¯•å¹¶å‘è¯·æ±‚"""
        print(f"\nğŸ”„ å¼€å§‹å¹¶å‘æµ‹è¯• ({num_requests} ä¸ªè¯·æ±‚)...")

        tasks = []
        for i in range(num_requests):
            prompt = random.choice(self.test_prompts)
            task = self.test_single_request(prompt, f"concurrent_{i}")
            tasks.append(task)

        start_time = time.time()
        results = await asyncio.gather(*tasks)
        total_time = time.time() - start_time

        successful = sum(1 for r in results if r.success)
        print(f"âœ… å¹¶å‘æµ‹è¯•å®Œæˆ: {successful}/{num_requests} æˆåŠŸ")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“Š ååé‡: {num_requests / total_time:.2f} è¯·æ±‚/ç§’")

        return results

    def test_sequential_requests(self, num_requests: int = 10) -> List[TestResult]:
        """æµ‹è¯•é¡ºåºè¯·æ±‚"""
        print(f"\nğŸ”„ å¼€å§‹é¡ºåºæµ‹è¯• ({num_requests} ä¸ªè¯·æ±‚)...")

        results = []
        start_time = time.time()

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        for i in range(num_requests):
            prompt = random.choice(self.test_prompts)
            result = loop.run_until_complete(
                self.test_single_request(prompt, f"sequential_{i}")
            )
            results.append(result)
            print(f"  è¯·æ±‚ {i+1}/{num_requests} å®Œæˆ - "
                  f"{'âœ… æˆåŠŸ' if result.success else 'âŒ å¤±è´¥'} "
                  f"({result.generation_time:.2f}ç§’)")

        total_time = time.time() - start_time
        loop.close()

        successful = sum(1 for r in results if r.success)
        print(f"âœ… é¡ºåºæµ‹è¯•å®Œæˆ: {successful}/{num_requests} æˆåŠŸ")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")

        return results

    def test_cache_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
        print("\nğŸ”„ å¼€å§‹ç¼“å­˜æ€§èƒ½æµ‹è¯•...")

        # ä½¿ç”¨ç›¸åŒçš„æç¤ºè¯æµ‹è¯•ç¼“å­˜
        test_prompt = "ç¼“å­˜æµ‹è¯•ï¼šä¸“ä¸šå•†åŠ¡æ¼”ç¤ºèƒŒæ™¯ï¼Œé«˜è´¨é‡4K"
        results = []

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆå†·ç¼“å­˜ï¼‰
        print("  æµ‹è¯•å†·ç¼“å­˜...")
        cold_result = loop.run_until_complete(
            self.test_single_request(test_prompt, "cache_cold")
        )
        results.append(cold_result)

        # åç»­è¯·æ±‚ï¼ˆçƒ­ç¼“å­˜ï¼‰
        print("  æµ‹è¯•çƒ­ç¼“å­˜...")
        for i in range(5):
            hot_result = loop.run_until_complete(
                self.test_single_request(test_prompt, f"cache_hot_{i}")
            )
            results.append(hot_result)

        loop.close()

        # åˆ†æç»“æœ
        cold_time = cold_result.generation_time
        hot_times = [r.generation_time for r in results[1:]]
        avg_hot_time = statistics.mean(hot_times)

        speedup = cold_time / avg_hot_time if avg_hot_time > 0 else 0
        cache_hits = sum(1 for r in results if r.from_cache)

        print(f"âœ… ç¼“å­˜æµ‹è¯•å®Œæˆ:")
        print(f"  å†·ç¼“å­˜æ—¶é—´: {cold_time:.2f}ç§’")
        print(f"  çƒ­ç¼“å­˜å¹³å‡æ—¶é—´: {avg_hot_time:.2f}ç§’")
        print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
        print(f"  ç¼“å­˜å‘½ä¸­: {cache_hits}/{len(results)}")

        return {
            'cold_time': cold_time,
            'avg_hot_time': avg_hot_time,
            'speedup': speedup,
            'cache_hits': cache_hits,
            'total_requests': len(results)
        }

    async def test_load_pattern(self,
                               pattern: str = "steady",
                               duration: int = 60,
                               base_rps: float = 1.0) -> List[TestResult]:
        """
        æµ‹è¯•ä¸åŒè´Ÿè½½æ¨¡å¼

        Args:
            pattern: è´Ÿè½½æ¨¡å¼ (steady, spike, gradual)
            duration: æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
            base_rps: åŸºç¡€è¯·æ±‚é€Ÿç‡ï¼ˆè¯·æ±‚/ç§’ï¼‰
        """
        print(f"\nğŸ”„ å¼€å§‹è´Ÿè½½æ¨¡å¼æµ‹è¯• (æ¨¡å¼: {pattern}, æŒç»­: {duration}ç§’)...")

        results = []
        start_time = time.time()
        current_time = 0

        while current_time < duration:
            # æ ¹æ®æ¨¡å¼è®¡ç®—å½“å‰RPS
            if pattern == "steady":
                current_rps = base_rps
            elif pattern == "spike":
                # åœ¨ä¸­é—´äº§ç”Ÿå°–å³°
                if duration * 0.4 < current_time < duration * 0.6:
                    current_rps = base_rps * 5
                else:
                    current_rps = base_rps
            elif pattern == "gradual":
                # é€æ¸å¢åŠ 
                current_rps = base_rps * (1 + current_time / duration * 2)
            else:
                current_rps = base_rps

            # è®¡ç®—è¿™ä¸€ç§’éœ€è¦å‘é€çš„è¯·æ±‚æ•°
            requests_this_second = int(current_rps)

            # å‘é€è¯·æ±‚
            tasks = []
            for _ in range(requests_this_second):
                prompt = random.choice(self.test_prompts)
                task = self.test_single_request(
                    prompt,
                    f"load_{pattern}_{int(current_time)}"
                )
                tasks.append(task)

            if tasks:
                batch_results = await asyncio.gather(*tasks)
                results.extend(batch_results)

            # ç­‰å¾…åˆ°ä¸‹ä¸€ç§’
            await asyncio.sleep(1)
            current_time = time.time() - start_time

        print(f"âœ… è´Ÿè½½æµ‹è¯•å®Œæˆ: å…±å‘é€ {len(results)} ä¸ªè¯·æ±‚")
        return results

    def test_model_fallback(self) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹æ•…éšœè½¬ç§»"""
        print("\nğŸ”„ å¼€å§‹æ¨¡å‹æ•…éšœè½¬ç§»æµ‹è¯•...")

        # æš‚æ—¶ç¦ç”¨ä¸»æ¨¡å‹ä»¥æµ‹è¯•fallback
        original_models = self.service.model_configs.copy()

        # æ¨¡æ‹ŸNovaæ¨¡å‹å¤±è´¥
        if "amazon.nova-canvas-v1:0" in self.service.model_configs:
            self.service.model_configs["amazon.nova-canvas-v1:0"].last_failure_time = datetime.now()

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # æµ‹è¯•fallback
        result = loop.run_until_complete(
            self.test_single_request(
                "æµ‹è¯•æ•…éšœè½¬ç§»ï¼šä¸“ä¸šæ¼”ç¤ºèƒŒæ™¯",
                "model_fallback"
            )
        )

        # æ¢å¤é…ç½®
        self.service.model_configs = original_models
        loop.close()

        print(f"âœ… æ•…éšœè½¬ç§»æµ‹è¯•å®Œæˆ:")
        print(f"  ä½¿ç”¨æ¨¡å‹: {result.model_used}")
        print(f"  è¯·æ±‚{'æˆåŠŸ' if result.success else 'å¤±è´¥'}")
        print(f"  è€—æ—¶: {result.generation_time:.2f}ç§’")

        return {
            'success': result.success,
            'model_used': result.model_used,
            'generation_time': result.generation_time
        }

    def calculate_metrics(self, results: List[TestResult]) -> PerformanceMetrics:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        if not results:
            return None

        successful_results = [r for r in results if r.success]
        times = [r.generation_time for r in successful_results]

        if not times:
            times = [0]

        sorted_times = sorted(times)
        p95_index = int(len(sorted_times) * 0.95)
        p99_index = int(len(sorted_times) * 0.99)

        metrics = PerformanceMetrics(
            total_requests=len(results),
            successful_requests=len(successful_results),
            failed_requests=len(results) - len(successful_results),
            cache_hits=sum(1 for r in results if r.from_cache),
            min_time=min(times) if times else 0,
            max_time=max(times) if times else 0,
            avg_time=statistics.mean(times) if times else 0,
            median_time=statistics.median(times) if times else 0,
            p95_time=sorted_times[p95_index] if p95_index < len(sorted_times) else 0,
            p99_time=sorted_times[p99_index] if p99_index < len(sorted_times) else 0,
            throughput=len(results) / sum(times) if sum(times) > 0 else 0,
            error_rate=(len(results) - len(successful_results)) / len(results) * 100,
            cache_hit_rate=sum(1 for r in results if r.from_cache) / len(results) * 100
        )

        return metrics

    def generate_report(self, output_dir: str = "performance_reports"):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        os.makedirs(output_dir, exist_ok=True)

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        metrics = self.calculate_metrics(self.test_results)

        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        report_path = os.path.join(output_dir, f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        with open(report_path, 'w') as f:
            f.write("=" * 80 + "\n")
            f.write("AI-PPT-Assistant å›¾ç‰‡ç”ŸæˆæœåŠ¡æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")

            f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»è¯·æ±‚æ•°: {metrics.total_requests}\n")
            f.write(f"æˆåŠŸè¯·æ±‚: {metrics.successful_requests}\n")
            f.write(f"å¤±è´¥è¯·æ±‚: {metrics.failed_requests}\n\n")

            f.write("æ€§èƒ½æŒ‡æ ‡:\n")
            f.write(f"  æœ€å°å“åº”æ—¶é—´: {metrics.min_time:.3f}ç§’\n")
            f.write(f"  æœ€å¤§å“åº”æ—¶é—´: {metrics.max_time:.3f}ç§’\n")
            f.write(f"  å¹³å‡å“åº”æ—¶é—´: {metrics.avg_time:.3f}ç§’\n")
            f.write(f"  ä¸­ä½æ•°å“åº”æ—¶é—´: {metrics.median_time:.3f}ç§’\n")
            f.write(f"  P95å“åº”æ—¶é—´: {metrics.p95_time:.3f}ç§’\n")
            f.write(f"  P99å“åº”æ—¶é—´: {metrics.p99_time:.3f}ç§’\n\n")

            f.write(f"ååé‡: {metrics.throughput:.2f} è¯·æ±‚/ç§’\n")
            f.write(f"é”™è¯¯ç‡: {metrics.error_rate:.2f}%\n")
            f.write(f"ç¼“å­˜å‘½ä¸­ç‡: {metrics.cache_hit_rate:.2f}%\n\n")

            # æŒ‰æµ‹è¯•ç±»å‹åˆ†ç»„ç»Ÿè®¡
            test_types = {}
            for result in self.test_results:
                test_type = result.test_name.split('_')[0]
                if test_type not in test_types:
                    test_types[test_type] = []
                test_types[test_type].append(result)

            f.write("æŒ‰æµ‹è¯•ç±»å‹ç»Ÿè®¡:\n")
            for test_type, type_results in test_types.items():
                type_metrics = self.calculate_metrics(type_results)
                f.write(f"\n  {test_type}:\n")
                f.write(f"    è¯·æ±‚æ•°: {type_metrics.total_requests}\n")
                f.write(f"    æˆåŠŸç‡: {(type_metrics.successful_requests/type_metrics.total_requests*100):.2f}%\n")
                f.write(f"    å¹³å‡æ—¶é—´: {type_metrics.avg_time:.3f}ç§’\n")

        print(f"âœ… æ–‡æœ¬æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

        # ç”Ÿæˆå›¾è¡¨
        self.generate_charts(output_dir, metrics)

        # ç”ŸæˆJSONæŠ¥å‘Š
        json_path = os.path.join(output_dir, f"performance_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(json_path, 'w') as f:
            json.dump(asdict(metrics), f, indent=2, default=str)

        print(f"âœ… JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {json_path}")

    def generate_charts(self, output_dir: str, metrics: PerformanceMetrics):
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
        plt.style.use('seaborn-v0_8-darkgrid')
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))

        # 1. å“åº”æ—¶é—´åˆ†å¸ƒ
        times = [r.generation_time for r in self.test_results if r.success]
        axes[0, 0].hist(times, bins=20, edgecolor='black')
        axes[0, 0].set_title('å“åº”æ—¶é—´åˆ†å¸ƒ')
        axes[0, 0].set_xlabel('æ—¶é—´ (ç§’)')
        axes[0, 0].set_ylabel('é¢‘æ¬¡')
        axes[0, 0].axvline(metrics.avg_time, color='red', linestyle='--', label=f'å¹³å‡: {metrics.avg_time:.2f}s')
        axes[0, 0].axvline(metrics.p95_time, color='orange', linestyle='--', label=f'P95: {metrics.p95_time:.2f}s')
        axes[0, 0].legend()

        # 2. æ—¶é—´åºåˆ—å›¾
        timestamps = [r.timestamp for r in self.test_results]
        response_times = [r.generation_time for r in self.test_results]
        axes[0, 1].plot(timestamps, response_times, 'b-', alpha=0.5)
        axes[0, 1].scatter(timestamps, response_times, c=['green' if r.success else 'red' for r in self.test_results], s=10)
        axes[0, 1].set_title('å“åº”æ—¶é—´è¶‹åŠ¿')
        axes[0, 1].set_xlabel('æ—¶é—´')
        axes[0, 1].set_ylabel('å“åº”æ—¶é—´ (ç§’)')
        axes[0, 1].tick_params(axis='x', rotation=45)

        # 3. æˆåŠŸç‡é¥¼å›¾
        success_data = [metrics.successful_requests, metrics.failed_requests]
        labels = ['æˆåŠŸ', 'å¤±è´¥']
        colors = ['#2ecc71', '#e74c3c']
        axes[0, 2].pie(success_data, labels=labels, colors=colors, autopct='%1.1f%%')
        axes[0, 2].set_title('è¯·æ±‚æˆåŠŸç‡')

        # 4. ç¼“å­˜å‘½ä¸­ç‡
        cache_data = [metrics.cache_hits, metrics.total_requests - metrics.cache_hits]
        labels = ['ç¼“å­˜å‘½ä¸­', 'ç¼“å­˜æœªå‘½ä¸­']
        colors = ['#3498db', '#95a5a6']
        axes[1, 0].pie(cache_data, labels=labels, colors=colors, autopct='%1.1f%%')
        axes[1, 0].set_title('ç¼“å­˜å‘½ä¸­ç‡')

        # 5. æ¨¡å‹ä½¿ç”¨åˆ†å¸ƒ
        model_usage = {}
        for r in self.test_results:
            if r.model_used not in model_usage:
                model_usage[r.model_used] = 0
            model_usage[r.model_used] += 1

        axes[1, 1].bar(model_usage.keys(), model_usage.values())
        axes[1, 1].set_title('æ¨¡å‹ä½¿ç”¨åˆ†å¸ƒ')
        axes[1, 1].set_xlabel('æ¨¡å‹')
        axes[1, 1].set_ylabel('ä½¿ç”¨æ¬¡æ•°')
        axes[1, 1].tick_params(axis='x', rotation=45)

        # 6. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        metrics_data = {
            'Min': metrics.min_time,
            'Avg': metrics.avg_time,
            'Median': metrics.median_time,
            'P95': metrics.p95_time,
            'P99': metrics.p99_time,
            'Max': metrics.max_time
        }
        axes[1, 2].bar(metrics_data.keys(), metrics_data.values(), color='skyblue')
        axes[1, 2].set_title('å“åº”æ—¶é—´æŒ‡æ ‡')
        axes[1, 2].set_xlabel('æŒ‡æ ‡')
        axes[1, 2].set_ylabel('æ—¶é—´ (ç§’)')

        plt.suptitle('AI-PPT-Assistant å›¾ç‰‡ç”ŸæˆæœåŠ¡æ€§èƒ½åˆ†æ', fontsize=16)
        plt.tight_layout()

        chart_path = os.path.join(output_dir, f"performance_charts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ… æ€§èƒ½å›¾è¡¨å·²ç”Ÿæˆ: {chart_path}")


async def run_comprehensive_test():
    """è¿è¡Œå…¨é¢çš„æ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹AI-PPT-Assistantå›¾ç‰‡ç”ŸæˆæœåŠ¡æ€§èƒ½æµ‹è¯•")
    print("=" * 60)

    tester = ImagePerformanceTester()

    # 1. åŸºå‡†æµ‹è¯•
    print("\nğŸ“Š 1. åŸºå‡†æµ‹è¯•")
    baseline_results = tester.test_sequential_requests(5)

    # 2. å¹¶å‘æµ‹è¯•
    print("\nğŸ“Š 2. å¹¶å‘æµ‹è¯•")
    concurrent_results = await tester.test_concurrent_requests(20)

    # 3. ç¼“å­˜æµ‹è¯•
    print("\nğŸ“Š 3. ç¼“å­˜æ€§èƒ½æµ‹è¯•")
    cache_results = tester.test_cache_performance()

    # 4. è´Ÿè½½æ¨¡å¼æµ‹è¯•
    print("\nğŸ“Š 4. è´Ÿè½½æ¨¡å¼æµ‹è¯•")
    print("  4.1 ç¨³å®šè´Ÿè½½")
    steady_results = await tester.test_load_pattern("steady", 30, 2.0)

    print("  4.2 å°–å³°è´Ÿè½½")
    spike_results = await tester.test_load_pattern("spike", 30, 1.0)

    print("  4.3 æ¸è¿›è´Ÿè½½")
    gradual_results = await tester.test_load_pattern("gradual", 30, 1.0)

    # 5. æ•…éšœè½¬ç§»æµ‹è¯•
    print("\nğŸ“Š 5. æ¨¡å‹æ•…éšœè½¬ç§»æµ‹è¯•")
    fallback_results = tester.test_model_fallback()

    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    tester.generate_report()

    # è¾“å‡ºæ€»ç»“
    all_metrics = tester.calculate_metrics(tester.test_results)
    print("\n" + "=" * 60)
    print("ğŸ¯ æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"æ€»è¯·æ±‚æ•°: {all_metrics.total_requests}")
    print(f"æˆåŠŸç‡: {(all_metrics.successful_requests/all_metrics.total_requests*100):.2f}%")
    print(f"å¹³å‡å“åº”æ—¶é—´: {all_metrics.avg_time:.3f}ç§’")
    print(f"P95å“åº”æ—¶é—´: {all_metrics.p95_time:.3f}ç§’")
    print(f"ååé‡: {all_metrics.throughput:.2f} è¯·æ±‚/ç§’")
    print(f"ç¼“å­˜å‘½ä¸­ç‡: {all_metrics.cache_hit_rate:.2f}%")

    # æ€§èƒ½å»ºè®®
    print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    if all_metrics.cache_hit_rate < 30:
        print("  âš ï¸ ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")
    if all_metrics.p95_time > 5:
        print("  âš ï¸ P95å“åº”æ—¶é—´è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–æ…¢è¯·æ±‚")
    if all_metrics.error_rate > 5:
        print("  âš ï¸ é”™è¯¯ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥æœåŠ¡ç¨³å®šæ€§")
    if all_metrics.throughput < 1:
        print("  âš ï¸ ååé‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ å¹¶å‘å¤„ç†èƒ½åŠ›")

    print("\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AI-PPT-Assistantå›¾ç‰‡ç”ŸæˆæœåŠ¡æ€§èƒ½æµ‹è¯•")
    parser.add_argument("--quick", action="store_true", help="è¿è¡Œå¿«é€Ÿæµ‹è¯•")
    parser.add_argument("--concurrent", type=int, help="å¹¶å‘æµ‹è¯•è¯·æ±‚æ•°")
    parser.add_argument("--duration", type=int, default=60, help="è´Ÿè½½æµ‹è¯•æŒç»­æ—¶é—´")

    args = parser.parse_args()

    if args.quick:
        # å¿«é€Ÿæµ‹è¯•
        tester = ImagePerformanceTester()
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        print("ğŸš€ è¿è¡Œå¿«é€Ÿæ€§èƒ½æµ‹è¯•...")
        loop.run_until_complete(tester.test_concurrent_requests(5))
        tester.generate_report()
        loop.close()
    else:
        # å®Œæ•´æµ‹è¯•
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(run_comprehensive_test())
        loop.close()