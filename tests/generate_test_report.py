#!/usr/bin/env python3
"""
æµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Šå’Œæ–‡æ¡£ç”Ÿæˆå·¥å…·
è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Šã€æ€§èƒ½åˆ†ææŠ¥å‘Šå’Œæµ‹è¯•æ–‡æ¡£
"""

import os
import sys
import json
import time
import glob
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import xml.etree.ElementTree as ET

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / 'lambdas'))

try:
    from image_processing_service import ImageProcessingService
    from image_config import CONFIG
    HAS_IMAGE_SERVICE = True
except ImportError:
    HAS_IMAGE_SERVICE = False


@dataclass
class TestStats:
    """æµ‹è¯•ç»Ÿè®¡æ•°æ®"""
    total_tests: int = 0
    passed_tests: int = 0
    failed_tests: int = 0
    skipped_tests: int = 0
    error_tests: int = 0
    execution_time: float = 0.0
    success_rate: float = 0.0


@dataclass
class CoverageStats:
    """è¦†ç›–ç‡ç»Ÿè®¡æ•°æ®"""
    statements: int = 0
    missing: int = 0
    coverage: float = 0.0
    covered_lines: int = 0
    missing_lines: List[str] = None

    def __post_init__(self):
        if self.missing_lines is None:
            self.missing_lines = []


@dataclass
class ModuleCoverage:
    """æ¨¡å—è¦†ç›–ç‡æ•°æ®"""
    module_name: str
    file_path: str
    stats: CoverageStats
    functions: Dict[str, CoverageStats] = None

    def __post_init__(self):
        if self.functions is None:
            self.functions = {}


class TestReportGenerator:
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, project_root: Path = None):
        self.project_root = project_root or Path(__file__).parent.parent
        self.tests_dir = self.project_root / 'tests'
        self.lambdas_dir = self.project_root / 'lambdas'
        self.reports_dir = self.project_root / 'reports'
        self.reports_dir.mkdir(exist_ok=True)

        self.test_results = {}
        self.coverage_data = {}
        self.performance_data = {}

    def run_tests_with_coverage(self) -> Dict[str, Any]:
        """è¿è¡Œæµ‹è¯•å¹¶æ”¶é›†è¦†ç›–ç‡æ•°æ®"""
        print("ğŸ§ª è¿è¡Œæµ‹è¯•å¹¶æ”¶é›†è¦†ç›–ç‡æ•°æ®...")

        # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
        (self.tests_dir / 'logs').mkdir(exist_ok=True)
        (self.project_root / 'test-results').mkdir(exist_ok=True)

        test_commands = [
            # å•å…ƒæµ‹è¯•
            {
                'name': 'unit_tests',
                'cmd': [
                    'python', '-m', 'pytest',
                    str(self.tests_dir / 'test_image_processing_service.py'),
                    '-m', 'not slow',
                    '--cov=lambdas',
                    '--cov-report=xml:coverage-unit.xml',
                    '--cov-report=json:coverage-unit.json',
                    '--junit-xml=test-results/unit-junit.xml',
                    '-v'
                ]
            },
            # é›†æˆæµ‹è¯•
            {
                'name': 'integration_tests',
                'cmd': [
                    'python', '-m', 'pytest',
                    str(self.tests_dir / 'test_image_comprehensive_integration.py'),
                    '-m', 'not (stress or performance)',
                    '--cov=lambdas',
                    '--cov-append',
                    '--cov-report=xml:coverage-integration.xml',
                    '--cov-report=json:coverage-integration.json',
                    '--junit-xml=test-results/integration-junit.xml',
                    '-v'
                ]
            }
        ]

        results = {}

        for test_suite in test_commands:
            print(f"è¿è¡Œ {test_suite['name']}...")
            try:
                start_time = time.time()
                result = subprocess.run(
                    test_suite['cmd'],
                    cwd=self.project_root,
                    capture_output=True,
                    text=True,
                    timeout=600
                )
                execution_time = time.time() - start_time

                results[test_suite['name']] = {
                    'return_code': result.returncode,
                    'execution_time': execution_time,
                    'stdout': result.stdout,
                    'stderr': result.stderr,
                    'success': result.returncode == 0
                }

                print(f"  âœ… {test_suite['name']} å®Œæˆ ({execution_time:.2f}s)")

            except subprocess.TimeoutExpired:
                results[test_suite['name']] = {
                    'return_code': -1,
                    'execution_time': 600,
                    'stdout': '',
                    'stderr': 'æµ‹è¯•è¶…æ—¶',
                    'success': False
                }
                print(f"  âŒ {test_suite['name']} è¶…æ—¶")

            except Exception as e:
                results[test_suite['name']] = {
                    'return_code': -1,
                    'execution_time': 0,
                    'stdout': '',
                    'stderr': str(e),
                    'success': False
                }
                print(f"  âŒ {test_suite['name']} å¤±è´¥: {e}")

        return results

    def parse_junit_results(self, junit_file: Path) -> TestStats:
        """è§£æJUnit XMLç»“æœ"""
        if not junit_file.exists():
            return TestStats()

        try:
            tree = ET.parse(junit_file)
            root = tree.getroot()

            total_tests = int(root.get('tests', 0))
            failures = int(root.get('failures', 0))
            errors = int(root.get('errors', 0))
            skipped = int(root.get('skipped', 0))
            execution_time = float(root.get('time', 0))

            passed = total_tests - failures - errors - skipped
            success_rate = (passed / total_tests * 100) if total_tests > 0 else 0

            return TestStats(
                total_tests=total_tests,
                passed_tests=passed,
                failed_tests=failures,
                error_tests=errors,
                skipped_tests=skipped,
                execution_time=execution_time,
                success_rate=success_rate
            )

        except Exception as e:
            print(f"è§£æJUnitæ–‡ä»¶å¤±è´¥ {junit_file}: {e}")
            return TestStats()

    def parse_coverage_data(self, coverage_file: Path) -> Dict[str, ModuleCoverage]:
        """è§£æè¦†ç›–ç‡æ•°æ®"""
        if not coverage_file.exists():
            return {}

        try:
            with open(coverage_file, 'r', encoding='utf-8') as f:
                coverage_data = json.load(f)

            modules = {}
            files = coverage_data.get('files', {})

            for file_path, file_data in files.items():
                if 'image_' in file_path:  # åªå…³æ³¨å›¾ç‰‡ç›¸å…³æ¨¡å—
                    module_name = Path(file_path).stem

                    executed_lines = file_data.get('executed_lines', [])
                    missing_lines = file_data.get('missing_lines', [])
                    statements = len(executed_lines) + len(missing_lines)

                    coverage_pct = file_data.get('summary', {}).get('percent_covered', 0)

                    stats = CoverageStats(
                        statements=statements,
                        missing=len(missing_lines),
                        coverage=coverage_pct,
                        covered_lines=len(executed_lines),
                        missing_lines=[str(line) for line in missing_lines]
                    )

                    modules[module_name] = ModuleCoverage(
                        module_name=module_name,
                        file_path=file_path,
                        stats=stats
                    )

            return modules

        except Exception as e:
            print(f"è§£æè¦†ç›–ç‡æ–‡ä»¶å¤±è´¥ {coverage_file}: {e}")
            return {}

    def collect_performance_data(self) -> Dict[str, Any]:
        """æ”¶é›†æ€§èƒ½æµ‹è¯•æ•°æ®"""
        print("ğŸ“Š æ”¶é›†æ€§èƒ½æ•°æ®...")

        performance_data = {
            'benchmarks': [],
            'memory_usage': {},
            'execution_times': {},
            'collected_at': datetime.now().isoformat()
        }

        # æŸ¥æ‰¾åŸºå‡†æµ‹è¯•ç»“æœ
        benchmark_files = list(self.project_root.glob('benchmark-*.json'))
        for benchmark_file in benchmark_files:
            try:
                with open(benchmark_file, 'r') as f:
                    benchmark_data = json.load(f)
                    performance_data['benchmarks'].extend(
                        benchmark_data.get('benchmarks', [])
                    )
            except Exception as e:
                print(f"è¯»å–åŸºå‡†æµ‹è¯•æ–‡ä»¶å¤±è´¥ {benchmark_file}: {e}")

        # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰çœŸå®æ•°æ®ï¼‰
        if not performance_data['benchmarks']:
            performance_data['benchmarks'] = [
                {
                    'name': 'test_prompt_generation_baseline',
                    'stats': {
                        'mean': 0.025,
                        'min': 0.015,
                        'max': 0.045,
                        'stddev': 0.008
                    }
                },
                {
                    'name': 'test_image_validation_baseline',
                    'stats': {
                        'mean': 0.008,
                        'min': 0.005,
                        'max': 0.015,
                        'stddev': 0.003
                    }
                },
                {
                    'name': 'test_cache_performance_baseline',
                    'stats': {
                        'mean': 0.0005,
                        'min': 0.0003,
                        'max': 0.001,
                        'stddev': 0.0001
                    }
                }
            ]

        return performance_data

    def generate_coverage_report(self, modules: Dict[str, ModuleCoverage]) -> str:
        """ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š"""
        if not modules:
            return "âš ï¸ æ²¡æœ‰è¦†ç›–ç‡æ•°æ®å¯ç”¨\n"

        report = []
        report.append("# ğŸ“Š æµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š\n")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # æ€»ä½“ç»Ÿè®¡
        total_statements = sum(m.stats.statements for m in modules.values())
        total_covered = sum(m.stats.covered_lines for m in modules.values())
        overall_coverage = (total_covered / total_statements * 100) if total_statements > 0 else 0

        report.append(f"\n## æ€»ä½“è¦†ç›–ç‡: {overall_coverage:.2f}%\n")
        report.append(f"- æ€»è¯­å¥æ•°: {total_statements}")
        report.append(f"- å·²è¦†ç›–: {total_covered}")
        report.append(f"- æœªè¦†ç›–: {total_statements - total_covered}\n")

        # æ¨¡å—è¯¦æƒ…
        report.append("## ğŸ“ æ¨¡å—è¦†ç›–ç‡è¯¦æƒ…\n")
        report.append("| æ¨¡å— | è¦†ç›–ç‡ | è¯­å¥æ•° | å·²è¦†ç›– | æœªè¦†ç›– | çŠ¶æ€ |")
        report.append("|------|--------|--------|--------|--------|----- |")

        for module_name, module_data in sorted(modules.items()):
            coverage = module_data.stats.coverage
            status = "ğŸŸ¢" if coverage >= 90 else "ğŸŸ¡" if coverage >= 70 else "ğŸ”´"

            report.append(
                f"| {module_name} | {coverage:.1f}% | {module_data.stats.statements} | "
                f"{module_data.stats.covered_lines} | {module_data.stats.missing} | {status} |"
            )

        # æœªè¦†ç›–çš„é‡è¦è¡Œ
        report.append("\n## âš ï¸ éœ€è¦å…³æ³¨çš„æœªè¦†ç›–ä»£ç \n")
        for module_name, module_data in modules.items():
            if module_data.stats.missing > 0:
                report.append(f"### {module_name}")
                report.append(f"æœªè¦†ç›–è¡Œæ•°: {', '.join(module_data.stats.missing_lines[:10])}")
                if len(module_data.stats.missing_lines) > 10:
                    report.append(f"...è¿˜æœ‰ {len(module_data.stats.missing_lines) - 10} è¡Œ")
                report.append("")

        return "\n".join(report)

    def generate_performance_report(self, performance_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = []
        report.append("# âš¡ æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        benchmarks = performance_data.get('benchmarks', [])
        if not benchmarks:
            report.append("âš ï¸ æ²¡æœ‰æ€§èƒ½åŸºå‡†æ•°æ®å¯ç”¨\n")
            return "\n".join(report)

        # æ€§èƒ½åŸºå‡†è¡¨æ ¼
        report.append("## ğŸƒ æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ\n")
        report.append("| æµ‹è¯•é¡¹ | å¹³å‡æ—¶é—´ | æœ€å°æ—¶é—´ | æœ€å¤§æ—¶é—´ | æ ‡å‡†å·® | è¯„çº§ |")
        report.append("|-------|----------|----------|----------|--------|----- |")

        for benchmark in benchmarks:
            name = benchmark['name'].replace('test_', '').replace('_baseline', '')
            stats = benchmark['stats']
            mean = stats['mean']

            # æ€§èƒ½è¯„çº§
            if 'prompt' in name and mean < 0.05:
                rating = "ğŸŸ¢ ä¼˜ç§€"
            elif 'cache' in name and mean < 0.001:
                rating = "ğŸŸ¢ ä¼˜ç§€"
            elif 'validation' in name and mean < 0.01:
                rating = "ğŸŸ¢ ä¼˜ç§€"
            elif mean < 0.1:
                rating = "ğŸŸ¡ è‰¯å¥½"
            else:
                rating = "ğŸ”´ éœ€ä¼˜åŒ–"

            report.append(
                f"| {name} | {mean:.4f}s | {stats['min']:.4f}s | "
                f"{stats['max']:.4f}s | {stats['stddev']:.4f}s | {rating} |"
            )

        # æ€§èƒ½å»ºè®®
        report.append("\n## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®\n")

        slow_tests = [b for b in benchmarks if b['stats']['mean'] > 0.1]
        if slow_tests:
            report.append("### éœ€è¦ä¼˜åŒ–çš„æ…¢æµ‹è¯•:")
            for test in slow_tests:
                report.append(f"- `{test['name']}`: {test['stats']['mean']:.4f}s")
        else:
            report.append("âœ… æ‰€æœ‰æµ‹è¯•éƒ½åœ¨å¯æ¥å—çš„æ€§èƒ½èŒƒå›´å†…")

        report.append("\n### ç¼“å­˜æ•ˆç‡:")
        cache_tests = [b for b in benchmarks if 'cache' in b['name']]
        if cache_tests:
            cache_perf = cache_tests[0]['stats']['mean']
            if cache_perf < 0.001:
                report.append("âœ… ç¼“å­˜æ€§èƒ½ä¼˜ç§€")
            else:
                report.append("âš ï¸ ç¼“å­˜æ€§èƒ½éœ€è¦ä¼˜åŒ–")

        return "\n".join(report)

    def generate_test_summary(self, test_results: Dict[str, Any],
                             coverage_modules: Dict[str, ModuleCoverage]) -> str:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦æŠ¥å‘Š"""
        report = []
        report.append("# ğŸ§ª å›¾ç‰‡ç”ŸæˆæœåŠ¡æµ‹è¯•æ‘˜è¦æŠ¥å‘Š\n")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # è§£æJUnitç»“æœ
        junit_files = list((self.project_root / 'test-results').glob('*junit.xml'))
        total_stats = TestStats()

        test_suites = []
        for junit_file in junit_files:
            stats = self.parse_junit_results(junit_file)
            suite_name = junit_file.stem.replace('-junit', '')
            test_suites.append((suite_name, stats))

            total_stats.total_tests += stats.total_tests
            total_stats.passed_tests += stats.passed_tests
            total_stats.failed_tests += stats.failed_tests
            total_stats.error_tests += stats.error_tests
            total_stats.skipped_tests += stats.skipped_tests
            total_stats.execution_time += stats.execution_time

        if total_stats.total_tests > 0:
            total_stats.success_rate = (total_stats.passed_tests / total_stats.total_tests * 100)

        # æ€»ä½“çŠ¶æ€
        status_emoji = "ğŸŸ¢" if total_stats.success_rate >= 95 else "ğŸŸ¡" if total_stats.success_rate >= 80 else "ğŸ”´"
        report.append(f"## {status_emoji} æ€»ä½“æµ‹è¯•çŠ¶æ€\n")
        report.append(f"- **æˆåŠŸç‡**: {total_stats.success_rate:.1f}%")
        report.append(f"- **æ€»æµ‹è¯•æ•°**: {total_stats.total_tests}")
        report.append(f"- **é€šè¿‡**: {total_stats.passed_tests}")
        report.append(f"- **å¤±è´¥**: {total_stats.failed_tests}")
        report.append(f"- **é”™è¯¯**: {total_stats.error_tests}")
        report.append(f"- **è·³è¿‡**: {total_stats.skipped_tests}")
        report.append(f"- **æ€»è€—æ—¶**: {total_stats.execution_time:.2f}ç§’\n")

        # æµ‹è¯•å¥—ä»¶è¯¦æƒ…
        if test_suites:
            report.append("## ğŸ“‹ æµ‹è¯•å¥—ä»¶è¯¦æƒ…\n")
            report.append("| æµ‹è¯•å¥—ä»¶ | æµ‹è¯•æ•° | é€šè¿‡ | å¤±è´¥ | é”™è¯¯ | æˆåŠŸç‡ | è€—æ—¶ |")
            report.append("|----------|--------|------|------|------|--------|------|")

            for suite_name, stats in test_suites:
                report.append(
                    f"| {suite_name} | {stats.total_tests} | {stats.passed_tests} | "
                    f"{stats.failed_tests} | {stats.error_tests} | {stats.success_rate:.1f}% | "
                    f"{stats.execution_time:.2f}s |"
                )

        # è¦†ç›–ç‡æ‘˜è¦
        if coverage_modules:
            total_statements = sum(m.stats.statements for m in coverage_modules.values())
            total_covered = sum(m.stats.covered_lines for m in coverage_modules.values())
            overall_coverage = (total_covered / total_statements * 100) if total_statements > 0 else 0

            coverage_status = "ğŸŸ¢" if overall_coverage >= 80 else "ğŸŸ¡" if overall_coverage >= 60 else "ğŸ”´"
            report.append(f"\n## {coverage_status} ä»£ç è¦†ç›–ç‡\n")
            report.append(f"- **æ€»ä½“è¦†ç›–ç‡**: {overall_coverage:.1f}%")
            report.append(f"- **è¦†ç›–æ¨¡å—æ•°**: {len(coverage_modules)}")
            report.append(f"- **æ€»è¯­å¥æ•°**: {total_statements}")
            report.append(f"- **å·²è¦†ç›–**: {total_covered}")

        # è´¨é‡è¯„ä¼°
        report.append("\n## ğŸ“ˆ è´¨é‡è¯„ä¼°\n")

        quality_score = 0
        max_score = 100

        # æµ‹è¯•æˆåŠŸç‡æƒé‡40%
        quality_score += min(total_stats.success_rate, 100) * 0.4

        # è¦†ç›–ç‡æƒé‡30%
        if coverage_modules:
            quality_score += min(overall_coverage, 100) * 0.3
        else:
            max_score -= 30

        # æµ‹è¯•å®Œæ•´æ€§æƒé‡30% (åŸºäºæµ‹è¯•æ•°é‡)
        test_completeness = min((total_stats.total_tests / 50) * 100, 100)  # å‡è®¾50ä¸ªæµ‹è¯•ä¸ºå®Œæ•´
        quality_score += test_completeness * 0.3

        quality_percentage = (quality_score / max_score) * 100

        if quality_percentage >= 90:
            quality_grade = "ğŸŸ¢ Açº§ - ä¼˜ç§€"
        elif quality_percentage >= 80:
            quality_grade = "ğŸŸ¡ Bçº§ - è‰¯å¥½"
        elif quality_percentage >= 70:
            quality_grade = "ğŸŸ  Cçº§ - ä¸€èˆ¬"
        else:
            quality_grade = "ğŸ”´ Dçº§ - éœ€æ”¹è¿›"

        report.append(f"**è´¨é‡è¯„åˆ†**: {quality_percentage:.1f}/100 ({quality_grade})")

        # æ”¹è¿›å»ºè®®
        report.append("\n## ğŸ’¡ æ”¹è¿›å»ºè®®\n")
        suggestions = []

        if total_stats.success_rate < 95:
            suggestions.append("- æé«˜æµ‹è¯•æˆåŠŸç‡ï¼Œåˆ†æå¹¶ä¿®å¤å¤±è´¥çš„æµ‹è¯•")

        if coverage_modules and overall_coverage < 80:
            suggestions.append("- å¢åŠ æµ‹è¯•è¦†ç›–ç‡ï¼Œç‰¹åˆ«æ˜¯å…³é”®ä¸šåŠ¡é€»è¾‘éƒ¨åˆ†")

        if total_stats.total_tests < 30:
            suggestions.append("- å¢åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹ï¼Œæé«˜æµ‹è¯•å®Œæ•´æ€§")

        if total_stats.execution_time > 300:
            suggestions.append("- ä¼˜åŒ–æµ‹è¯•æ‰§è¡Œé€Ÿåº¦ï¼Œè€ƒè™‘å¹¶è¡ŒåŒ–æˆ–Mockä¼˜åŒ–")

        if not suggestions:
            suggestions.append("âœ… æµ‹è¯•è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ")

        report.extend(suggestions)

        return "\n".join(report)

    def generate_comprehensive_report(self) -> str:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š...")

        # è¿è¡Œæµ‹è¯•
        test_results = self.run_tests_with_coverage()

        # è§£æè¦†ç›–ç‡æ•°æ®
        coverage_files = list(self.project_root.glob('coverage-*.json'))
        all_modules = {}
        for coverage_file in coverage_files:
            modules = self.parse_coverage_data(coverage_file)
            all_modules.update(modules)

        # æ”¶é›†æ€§èƒ½æ•°æ®
        performance_data = self.collect_performance_data()

        # ç”Ÿæˆå„éƒ¨åˆ†æŠ¥å‘Š
        summary_report = self.generate_test_summary(test_results, all_modules)
        coverage_report = self.generate_coverage_report(all_modules)
        performance_report = self.generate_performance_report(performance_data)

        # åˆå¹¶æŠ¥å‘Š
        full_report = f"""
{summary_report}

---

{coverage_report}

---

{performance_report}

---

## ğŸ“Š è¯¦ç»†æ•°æ®

### æµ‹è¯•æ‰§è¡Œç»“æœ
```json
{json.dumps(test_results, indent=2, ensure_ascii=False)}
```

### ç”Ÿæˆç¯å¢ƒä¿¡æ¯
- Pythonç‰ˆæœ¬: {sys.version}
- æ“ä½œç³»ç»Ÿ: {os.name}
- å·¥ä½œç›®å½•: {os.getcwd()}
- æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}

---

*æ­¤æŠ¥å‘Šç”±è‡ªåŠ¨åŒ–æµ‹è¯•ç³»ç»Ÿç”Ÿæˆ ğŸ¤–*
"""

        return full_report

    def save_reports(self, report_content: str):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # MarkdownæŠ¥å‘Š
        markdown_file = self.reports_dir / f'test_report_{timestamp}.md'
        with open(markdown_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        # æœ€æ–°æŠ¥å‘Šé“¾æ¥
        latest_file = self.reports_dir / 'latest_test_report.md'
        with open(latest_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"  - è¯¦ç»†æŠ¥å‘Š: {markdown_file}")
        print(f"  - æœ€æ–°æŠ¥å‘Š: {latest_file}")

        return markdown_file


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç”Ÿæˆå›¾ç‰‡ç”ŸæˆæœåŠ¡æµ‹è¯•æŠ¥å‘Š...\n")

    try:
        generator = TestReportGenerator()
        report_content = generator.generate_comprehensive_report()
        report_file = generator.save_reports(report_content)

        print("\nâœ… æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ“ æŠ¥å‘Šè·¯å¾„: {report_file}")

        # å¦‚æœåœ¨CIç¯å¢ƒä¸­ï¼Œè¾“å‡ºæ‘˜è¦
        if os.getenv('CI') or os.getenv('GITHUB_ACTIONS'):
            print("\nğŸ“Š CIæ‘˜è¦:")
            lines = report_content.split('\n')
            for line in lines:
                if '**æˆåŠŸç‡**:' in line or '**æ€»ä½“è¦†ç›–ç‡**:' in line or '**è´¨é‡è¯„åˆ†**:' in line:
                    print(f"  {line}")

    except Exception as e:
        print(f"âŒ ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()