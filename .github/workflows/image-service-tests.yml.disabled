name: Image Service Integration Tests

on:
  push:
    branches: [ main, dev ]
    paths:
      - 'lambdas/image_*.py'
      - 'tests/test_image_*.py'
      - '.github/workflows/image-service-tests.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'lambdas/image_*.py'
      - 'tests/test_image_*.py'
  schedule:
    # æ¯å¤©å‡Œæ™¨2ç‚¹è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'æµ‹è¯•çº§åˆ«'
        required: true
        default: 'basic'
        type: choice
        options:
          - basic
          - comprehensive
          - stress
          - performance

env:
  PYTHON_VERSION: '3.13'
  AWS_REGION: 'us-east-1'
  PYTHONPATH: ${{ github.workspace }}

jobs:
  # åŸºç¡€æµ‹è¯•ä½œä¸š
  basic-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    strategy:
      matrix:
        python-version: ['3.11', '3.12', '3.13']

    steps:
    - name: Checkoutä»£ç 
      uses: actions/checkout@v4

    - name: è®¾ç½®Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: ç¼“å­˜ä¾èµ–
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('tests/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-

    - name: å®‰è£…ä¾èµ–
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements.txt
        pip install pytest-cov pytest-xdist pytest-timeout

    - name: å®‰è£…ç³»ç»Ÿä¾èµ–
      run: |
        sudo apt-get update
        sudo apt-get install -y libimage-dev libjpeg-dev libpng-dev

    - name: è¿è¡ŒåŸºç¡€æµ‹è¯•
      run: |
        pytest tests/test_image_processing_service.py \
               tests/test_image_generator.py::TestImageGenerator \
               -v \
               --cov=lambdas \
               --cov-report=xml \
               --cov-report=html \
               --junit-xml=test-results/junit.xml \
               --timeout=300 \
               -n auto

    - name: ä¸Šä¼ æµ‹è¯•ç»“æœ
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: basic-test-results-py${{ matrix.python-version }}
        path: |
          test-results/
          htmlcov/
          .coverage

    - name: ä¸Šä¼ è¦†ç›–ç‡åˆ°Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: basic-tests
        name: codecov-py${{ matrix.python-version }}

  # é›†æˆæµ‹è¯•ä½œä¸š
  integration-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: basic-tests

    steps:
    - name: Checkoutä»£ç 
      uses: actions/checkout@v4

    - name: è®¾ç½®Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: å®‰è£…ä¾èµ–
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements.txt
        pip install pytest-asyncio pytest-mock

    - name: é…ç½®AWSå‡­è¯ï¼ˆæ¨¡æ‹Ÿï¼‰
      run: |
        mkdir -p ~/.aws
        echo -e "[default]\naws_access_key_id = testing\naws_secret_access_key = testing" > ~/.aws/credentials
        echo -e "[default]\nregion = us-east-1\noutput = json" > ~/.aws/config

    - name: è¿è¡Œé›†æˆæµ‹è¯•
      run: |
        pytest tests/test_image_comprehensive_integration.py \
               -v \
               --cov=lambdas \
               --cov-append \
               --cov-report=xml \
               --junit-xml=test-results/integration-junit.xml \
               --timeout=600 \
               -m "not (stress or performance)"

    - name: ä¸Šä¼ é›†æˆæµ‹è¯•ç»“æœ
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: |
          test-results/
          .coverage

  # æ€§èƒ½åŸºå‡†æµ‹è¯•ä½œä¸š
  performance-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: integration-tests
    if: github.event_name == 'schedule' || github.event.inputs.test_level == 'performance' || github.event.inputs.test_level == 'comprehensive'

    steps:
    - name: Checkoutä»£ç 
      uses: actions/checkout@v4

    - name: è®¾ç½®Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: å®‰è£…ä¾èµ–
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements.txt
        pip install pytest-benchmark psutil

    - name: è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
      run: |
        pytest tests/test_image_performance_benchmarks.py \
               -v \
               --benchmark-json=performance-results.json \
               --junit-xml=test-results/performance-junit.xml \
               --timeout=1800 \
               -m "not (load or scalability)"

    - name: åˆ†ææ€§èƒ½ç»“æœ
      run: |
        python -c "
        import json
        with open('performance-results.json') as f:
            data = json.load(f)

        print('æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:')
        for benchmark in data['benchmarks']:
            name = benchmark['name']
            stats = benchmark['stats']
            print(f'{name}: å¹³å‡ {stats['mean']:.4f}s, æœ€å° {stats['min']:.4f}s, æœ€å¤§ {stats['max']:.4f}s')
        "

    - name: ä¸Šä¼ æ€§èƒ½æµ‹è¯•ç»“æœ
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: |
          performance-results.json
          test-results/

  # å‹åŠ›æµ‹è¯•ä½œä¸š
  stress-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: integration-tests
    if: github.event_name == 'schedule' || github.event.inputs.test_level == 'stress' || github.event.inputs.test_level == 'comprehensive'

    steps:
    - name: Checkoutä»£ç 
      uses: actions/checkout@v4

    - name: è®¾ç½®Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: å®‰è£…ä¾èµ–
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements.txt
        pip install pytest-xdist

    - name: è¿è¡Œå‹åŠ›æµ‹è¯•
      run: |
        pytest tests/test_image_stress_concurrency.py \
               -v \
               --junit-xml=test-results/stress-junit.xml \
               --timeout=3000 \
               -m "stress" \
               --maxfail=3

    - name: ä¸Šä¼ å‹åŠ›æµ‹è¯•ç»“æœ
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: stress-test-results
        path: test-results/

  # ç«¯åˆ°ç«¯æµ‹è¯•ä½œä¸š
  e2e-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [integration-tests]
    if: github.event_name == 'schedule' || github.event.inputs.test_level == 'comprehensive'

    services:
      # æ¨¡æ‹ŸAWS LocalStackæœåŠ¡
      localstack:
        image: localstack/localstack:latest
        ports:
          - 4566:4566
        env:
          SERVICES: s3,lambda,bedrock
          DEBUG: 1

    steps:
    - name: Checkoutä»£ç 
      uses: actions/checkout@v4

    - name: è®¾ç½®Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: å®‰è£…ä¾èµ–
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements.txt
        pip install awscli-local

    - name: ç­‰å¾…LocalStackå°±ç»ª
      run: |
        timeout 60 bash -c 'until curl -s http://localhost:4566/_localstack/health | grep -q "\"s3\": \"available\""; do sleep 2; done'

    - name: è®¾ç½®LocalStackç¯å¢ƒ
      run: |
        export AWS_ENDPOINT_URL=http://localhost:4566
        export AWS_ACCESS_KEY_ID=testing
        export AWS_SECRET_ACCESS_KEY=testing
        export AWS_DEFAULT_REGION=us-east-1

        # åˆ›å»ºæµ‹è¯•S3æ¡¶
        awslocal s3 mb s3://ai-ppt-presentations-test

    - name: è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•
      env:
        AWS_ENDPOINT_URL: http://localhost:4566
        AWS_ACCESS_KEY_ID: testing
        AWS_SECRET_ACCESS_KEY: testing
        AWS_DEFAULT_REGION: us-east-1
      run: |
        pytest tests/test_image_integration.py \
               -v \
               --junit-xml=test-results/e2e-junit.xml \
               --timeout=1200 \
               -m "e2e"

    - name: ä¸Šä¼ ç«¯åˆ°ç«¯æµ‹è¯•ç»“æœ
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: test-results/

  # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
  generate-report:
    runs-on: ubuntu-latest
    needs: [basic-tests, integration-tests, performance-tests, stress-tests, e2e-tests]
    if: always()

    steps:
    - name: Checkoutä»£ç 
      uses: actions/checkout@v4

    - name: ä¸‹è½½æ‰€æœ‰æµ‹è¯•ç»“æœ
      uses: actions/download-artifact@v3
      with:
        path: all-test-results/

    - name: è®¾ç½®Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š
      run: |
        python - << 'EOF'
        import os
        import json
        import xml.etree.ElementTree as ET
        from datetime import datetime
        import glob

        def parse_junit_xml(file_path):
            """è§£æJUnit XMLæ–‡ä»¶"""
            try:
                tree = ET.parse(file_path)
                root = tree.getroot()

                return {
                    'tests': int(root.get('tests', 0)),
                    'failures': int(root.get('failures', 0)),
                    'errors': int(root.get('errors', 0)),
                    'time': float(root.get('time', 0)),
                    'name': root.get('name', os.path.basename(file_path))
                }
            except Exception as e:
                print(f"è§£æ {file_path} å¤±è´¥: {e}")
                return None

        # æ”¶é›†æ‰€æœ‰JUnit XMLæ–‡ä»¶
        junit_files = glob.glob('all-test-results/**/junit*.xml', recursive=True)

        report = {
            'generated_at': datetime.now().isoformat(),
            'workflow_run': os.environ.get('GITHUB_RUN_NUMBER', 'local'),
            'commit_sha': os.environ.get('GITHUB_SHA', 'unknown'),
            'branch': os.environ.get('GITHUB_REF_NAME', 'unknown'),
            'test_suites': [],
            'summary': {
                'total_tests': 0,
                'total_failures': 0,
                'total_errors': 0,
                'total_time': 0,
                'success_rate': 0
            }
        }

        # è§£ææ‰€æœ‰æµ‹è¯•ç»“æœ
        for junit_file in junit_files:
            result = parse_junit_xml(junit_file)
            if result:
                report['test_suites'].append(result)
                report['summary']['total_tests'] += result['tests']
                report['summary']['total_failures'] += result['failures']
                report['summary']['total_errors'] += result['errors']
                report['summary']['total_time'] += result['time']

        # è®¡ç®—æˆåŠŸç‡
        total_issues = report['summary']['total_failures'] + report['summary']['total_errors']
        if report['summary']['total_tests'] > 0:
            report['summary']['success_rate'] = (
                (report['summary']['total_tests'] - total_issues) /
                report['summary']['total_tests']
            ) * 100

        # ä¿å­˜æŠ¥å‘Š
        with open('comprehensive_test_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        # æ‰“å°æ‘˜è¦
        print("=" * 60)
        print("å›¾ç‰‡ç”ŸæˆæœåŠ¡æµ‹è¯•æ‘˜è¦æŠ¥å‘Š")
        print("=" * 60)
        print(f"æ€»æµ‹è¯•æ•°: {report['summary']['total_tests']}")
        print(f"å¤±è´¥æ•°: {report['summary']['total_failures']}")
        print(f"é”™è¯¯æ•°: {report['summary']['total_errors']}")
        print(f"æˆåŠŸç‡: {report['summary']['success_rate']:.2f}%")
        print(f"æ€»è€—æ—¶: {report['summary']['total_time']:.2f}ç§’")
        print("=" * 60)

        for suite in report['test_suites']:
            suite_success_rate = ((suite['tests'] - suite['failures'] - suite['errors']) / suite['tests'] * 100) if suite['tests'] > 0 else 0
            print(f"  {suite['name']}: {suite['tests']}ä¸ªæµ‹è¯•, æˆåŠŸç‡ {suite_success_rate:.1f}%, è€—æ—¶ {suite['time']:.2f}s")
        EOF

    - name: ä¸Šä¼ ç»¼åˆæµ‹è¯•æŠ¥å‘Š
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-test-report
        path: comprehensive_test_report.json

    - name: è¯„è®ºPRï¼ˆå¦‚æœæ˜¯PRï¼‰
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = JSON.parse(fs.readFileSync('comprehensive_test_report.json', 'utf8'));

          const body = `## ğŸ§ª å›¾ç‰‡ç”ŸæˆæœåŠ¡æµ‹è¯•æŠ¥å‘Š

          **æµ‹è¯•æ‘˜è¦:**
          - ğŸ“Š æ€»æµ‹è¯•æ•°: ${report.summary.total_tests}
          - âœ… æˆåŠŸç‡: ${report.summary.success_rate.toFixed(2)}%
          - âŒ å¤±è´¥æ•°: ${report.summary.total_failures}
          - ğŸš¨ é”™è¯¯æ•°: ${report.summary.total_errors}
          - â±ï¸ æ€»è€—æ—¶: ${report.summary.total_time.toFixed(2)}ç§’

          **è¯¦ç»†ç»“æœ:**
          ${report.test_suites.map(suite => {
            const successRate = suite.tests > 0 ? ((suite.tests - suite.failures - suite.errors) / suite.tests * 100).toFixed(1) : 0;
            const status = successRate >= 95 ? 'âœ…' : successRate >= 80 ? 'âš ï¸' : 'âŒ';
            return \`\${status} \${suite.name}: \${suite.tests}ä¸ªæµ‹è¯•, æˆåŠŸç‡ \${successRate}%\`;
          }).join('\n')}

          ${report.summary.success_rate >= 95 ? 'ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!' : 'âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¯¦ç»†ç»“æœ'}`;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          });

  # è´¨é‡é—¨ç¦æ£€æŸ¥
  quality-gate:
    runs-on: ubuntu-latest
    needs: [basic-tests, integration-tests]
    if: always()

    steps:
    - name: æ£€æŸ¥æµ‹è¯•ç»“æœ
      run: |
        echo "æ£€æŸ¥è´¨é‡é—¨ç¦..."

        # æ£€æŸ¥åŸºç¡€æµ‹è¯•æ˜¯å¦æˆåŠŸ
        if [[ "${{ needs.basic-tests.result }}" != "success" ]]; then
          echo "âŒ åŸºç¡€æµ‹è¯•å¤±è´¥"
          exit 1
        fi

        # æ£€æŸ¥é›†æˆæµ‹è¯•æ˜¯å¦æˆåŠŸ
        if [[ "${{ needs.integration-tests.result }}" != "success" ]]; then
          echo "âŒ é›†æˆæµ‹è¯•å¤±è´¥"
          exit 1
        fi

        echo "âœ… è´¨é‡é—¨ç¦é€šè¿‡"

    - name: é€šçŸ¥æµ‹è¯•çŠ¶æ€
      if: failure()
      run: |
        echo "::error::è´¨é‡é—¨ç¦å¤±è´¥ - è¯·æ£€æŸ¥æµ‹è¯•ç»“æœ"