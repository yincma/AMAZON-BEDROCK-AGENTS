#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•è¿è¡Œè„šæœ¬
æ‰§è¡Œå…¨é¢çš„æ€§èƒ½æµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š
"""

import sys
import os
import time
import json
import asyncio
import argparse
from datetime import datetime
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'lambdas'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'tests'))

from lambdas.image_processing_service_v3 import ImageProcessingServiceV3, ImageRequest
from lambdas.cloudwatch_monitoring import CloudWatchMonitor, init_monitoring
from tests.test_performance_v3 import PerformanceBenchmark
import boto3
from botocore.exceptions import ClientError


class PerformanceTestRunner:
    """æ€§èƒ½æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.results = {}
        self.service = None
        self.monitor = None

    def setup(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        print("ğŸ”§ è®¾ç½®æµ‹è¯•ç¯å¢ƒ...")

        # åˆå§‹åŒ–æœåŠ¡
        if self.config.get('use_real_api'):
            bedrock_client = boto3.client('bedrock-runtime')
            s3_client = boto3.client('s3')
            cloudwatch_client = boto3.client('cloudwatch')
        else:
            # ä½¿ç”¨æ¨¡æ‹Ÿå®¢æˆ·ç«¯
            from unittest.mock import Mock, MagicMock
            bedrock_client = self._create_mock_bedrock_client()
            s3_client = Mock()
            cloudwatch_client = Mock()

        self.service = ImageProcessingServiceV3(
            bedrock_client=bedrock_client,
            s3_client=s3_client,
            cloudwatch_client=cloudwatch_client,
            enable_caching=self.config.get('enable_cache', True),
            enable_metrics=self.config.get('enable_metrics', True),
            max_workers=self.config.get('max_workers', 10)
        )

        # åˆå§‹åŒ–ç›‘æ§
        if self.config.get('enable_monitoring'):
            self.monitor = init_monitoring(
                namespace="AI-PPT-Assistant-Test",
                setup_alarms=False
            )

        print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")

    def _create_mock_bedrock_client(self):
        """åˆ›å»ºæ¨¡æ‹Ÿçš„Bedrockå®¢æˆ·ç«¯"""
        from unittest.mock import Mock, MagicMock

        client = Mock()

        def mock_invoke_model(**kwargs):
            # æ¨¡æ‹Ÿå»¶è¿Ÿ
            import time
            time.sleep(0.1)

            # è¿”å›æ¨¡æ‹Ÿå“åº”
            return {
                'body': MagicMock(
                    read=lambda: json.dumps({
                        'images': ['mock_base64_image_data']
                    })
                )
            }

        client.invoke_model = mock_invoke_model
        return client

    async def run_single_request_test(self, iterations: int = 10) -> Dict[str, Any]:
        """è¿è¡Œå•è¯·æ±‚æµ‹è¯•"""
        print(f"\nğŸ“Š è¿è¡Œå•è¯·æ±‚æµ‹è¯• ({iterations} æ¬¡è¿­ä»£)...")

        benchmark = PerformanceBenchmark()
        times = []

        for i in range(iterations):
            request = ImageRequest(
                prompt=f"é«˜è´¨é‡å•†åŠ¡æ¼”ç¤ºèƒŒæ™¯ æµ‹è¯•{i}",
                priority=1
            )

            start = time.perf_counter()
            response = await self.service._generate_single_async(request)
            elapsed = time.perf_counter() - start

            times.append(elapsed)
            print(f"  è¿­ä»£ {i+1}/{iterations}: {elapsed:.3f}ç§’")

        # è®¡ç®—ç»Ÿè®¡
        import statistics
        stats = {
            'test_name': 'single_request',
            'iterations': iterations,
            'mean_time': statistics.mean(times),
            'median_time': statistics.median(times),
            'min_time': min(times),
            'max_time': max(times),
            'std_dev': statistics.stdev(times) if len(times) > 1 else 0
        }

        print(f"âœ… å•è¯·æ±‚æµ‹è¯•å®Œæˆ: å¹³å‡ {stats['mean_time']:.3f}ç§’")
        return stats

    async def run_batch_test(self, batch_size: int = 10) -> Dict[str, Any]:
        """è¿è¡Œæ‰¹å¤„ç†æµ‹è¯•"""
        print(f"\nğŸ“Š è¿è¡Œæ‰¹å¤„ç†æµ‹è¯• (æ‰¹å¤§å°: {batch_size})...")

        requests = [
            ImageRequest(
                prompt=f"æ‰¹å¤„ç†æµ‹è¯•å›¾ç‰‡ {i}",
                priority=i % 3
            )
            for i in range(batch_size)
        ]

        start = time.perf_counter()
        responses = await self.service.generate_images_batch(requests)
        elapsed = time.perf_counter() - start

        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥
        successful = sum(1 for r in responses if r.from_cache or r.model_used != "placeholder")

        stats = {
            'test_name': 'batch_processing',
            'batch_size': batch_size,
            'total_time': elapsed,
            'avg_time_per_image': elapsed / batch_size,
            'successful': successful,
            'failed': batch_size - successful,
            'success_rate': successful / batch_size
        }

        print(f"âœ… æ‰¹å¤„ç†æµ‹è¯•å®Œæˆ: {batch_size}å¼ å›¾ç‰‡è€—æ—¶ {elapsed:.2f}ç§’")
        return stats

    async def run_concurrent_test(self, concurrent_requests: int = 20) -> Dict[str, Any]:
        """è¿è¡Œå¹¶å‘æµ‹è¯•"""
        print(f"\nğŸ“Š è¿è¡Œå¹¶å‘æµ‹è¯• ({concurrent_requests} ä¸ªå¹¶å‘è¯·æ±‚)...")

        async def make_request(index: int):
            request = ImageRequest(
                prompt=f"å¹¶å‘æµ‹è¯• {index}",
                priority=index % 5
            )
            start = time.perf_counter()
            try:
                response = await self.service._generate_single_async(request)
                return time.perf_counter() - start, True
            except Exception as e:
                return time.perf_counter() - start, False

        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [make_request(i) for i in range(concurrent_requests)]

        start = time.perf_counter()
        results = await asyncio.gather(*tasks)
        total_time = time.perf_counter() - start

        # åˆ†æç»“æœ
        times = [r[0] for r in results]
        successful = sum(1 for r in results if r[1])

        import statistics
        stats = {
            'test_name': 'concurrent_requests',
            'concurrent_count': concurrent_requests,
            'total_time': total_time,
            'mean_response_time': statistics.mean(times),
            'max_response_time': max(times),
            'successful': successful,
            'failed': concurrent_requests - successful,
            'success_rate': successful / concurrent_requests,
            'throughput': concurrent_requests / total_time
        }

        print(f"âœ… å¹¶å‘æµ‹è¯•å®Œæˆ: {concurrent_requests}ä¸ªè¯·æ±‚è€—æ—¶ {total_time:.2f}ç§’")
        print(f"   ååé‡: {stats['throughput']:.2f} req/s")
        return stats

    async def run_cache_test(self) -> Dict[str, Any]:
        """è¿è¡Œç¼“å­˜æµ‹è¯•"""
        print("\nğŸ“Š è¿è¡Œç¼“å­˜æ€§èƒ½æµ‹è¯•...")

        # ä½¿ç”¨ç›¸åŒçš„æç¤ºè¯æµ‹è¯•ç¼“å­˜
        prompt = "ç¼“å­˜æµ‹è¯•ä¸“ç”¨æç¤ºè¯"
        request = ImageRequest(prompt=prompt)

        # ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆå†·ç¼“å­˜ï¼‰
        start = time.perf_counter()
        response1 = await self.service._generate_single_async(request)
        cold_time = time.perf_counter() - start

        # ç¬¬äºŒæ¬¡è¯·æ±‚ï¼ˆçƒ­ç¼“å­˜ï¼‰
        start = time.perf_counter()
        response2 = await self.service._generate_single_async(request)
        hot_time = time.perf_counter() - start

        # æ‰¹é‡ç¼“å­˜æµ‹è¯•
        cache_hits = 0
        for i in range(10):
            response = await self.service._generate_single_async(request)
            if response.from_cache:
                cache_hits += 1

        stats = {
            'test_name': 'cache_performance',
            'cold_cache_time': cold_time,
            'hot_cache_time': hot_time,
            'speedup': cold_time / hot_time if hot_time > 0 else 0,
            'cache_hit_rate': cache_hits / 10,
            'cache_stats': self.service.cache_manager.get_stats()
        }

        print(f"âœ… ç¼“å­˜æµ‹è¯•å®Œæˆ:")
        print(f"   å†·ç¼“å­˜: {cold_time:.3f}ç§’")
        print(f"   çƒ­ç¼“å­˜: {hot_time:.3f}ç§’")
        print(f"   åŠ é€Ÿæ¯”: {stats['speedup']:.2f}x")
        print(f"   å‘½ä¸­ç‡: {stats['cache_hit_rate']*100:.1f}%")

        return stats

    async def run_stress_test(self, duration: int = 30) -> Dict[str, Any]:
        """è¿è¡Œå‹åŠ›æµ‹è¯•"""
        print(f"\nğŸ“Š è¿è¡Œå‹åŠ›æµ‹è¯• (æŒç»­ {duration} ç§’)...")

        start_time = time.time()
        request_count = 0
        successful = 0
        failed = 0
        response_times = []

        while time.time() - start_time < duration:
            request = ImageRequest(
                prompt=f"å‹åŠ›æµ‹è¯• {request_count}",
                priority=request_count % 3
            )

            req_start = time.perf_counter()
            try:
                response = await self.service._generate_single_async(request)
                successful += 1
            except Exception as e:
                failed += 1

            response_times.append(time.perf_counter() - req_start)
            request_count += 1

            # æ§åˆ¶è¯·æ±‚é€Ÿç‡
            await asyncio.sleep(0.1)

        elapsed = time.time() - start_time

        import statistics
        stats = {
            'test_name': 'stress_test',
            'duration': elapsed,
            'total_requests': request_count,
            'successful': successful,
            'failed': failed,
            'success_rate': successful / request_count if request_count > 0 else 0,
            'throughput': request_count / elapsed,
            'mean_response_time': statistics.mean(response_times),
            'p95_response_time': statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else max(response_times),
            'p99_response_time': statistics.quantiles(response_times, n=100)[98] if len(response_times) > 100 else max(response_times)
        }

        print(f"âœ… å‹åŠ›æµ‹è¯•å®Œæˆ:")
        print(f"   æ€»è¯·æ±‚: {request_count}")
        print(f"   æˆåŠŸç‡: {stats['success_rate']*100:.1f}%")
        print(f"   ååé‡: {stats['throughput']:.2f} req/s")
        print(f"   P95å»¶è¿Ÿ: {stats['p95_response_time']:.3f}ç§’")

        return stats

    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\nğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•å¥—ä»¶\n")
        print("=" * 50)

        all_results = {}

        # å•è¯·æ±‚æµ‹è¯•
        all_results['single_request'] = await self.run_single_request_test(
            iterations=self.config.get('single_iterations', 10)
        )

        # æ‰¹å¤„ç†æµ‹è¯•
        all_results['batch_processing'] = await self.run_batch_test(
            batch_size=self.config.get('batch_size', 10)
        )

        # å¹¶å‘æµ‹è¯•
        all_results['concurrent'] = await self.run_concurrent_test(
            concurrent_requests=self.config.get('concurrent_requests', 20)
        )

        # ç¼“å­˜æµ‹è¯•
        all_results['cache'] = await self.run_cache_test()

        # å‹åŠ›æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
        if self.config.get('run_stress_test', False):
            all_results['stress'] = await self.run_stress_test(
                duration=self.config.get('stress_duration', 30)
            )

        # è·å–æœåŠ¡ç»Ÿè®¡
        all_results['service_stats'] = self.service.get_performance_stats()

        print("\n" + "=" * 50)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")

        return all_results

    def generate_report(self, results: Dict[str, Any], output_file: str):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print(f"\nğŸ“ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š: {output_file}")

        report = {
            'timestamp': datetime.now().isoformat(),
            'config': self.config,
            'results': results,
            'summary': self._generate_summary(results)
        }

        # ä¿å­˜JSONæŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_file = output_file.replace('.json', '.html')
        self._generate_html_report(report, html_file)

        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"   JSON: {output_file}")
        print(f"   HTML: {html_file}")

    def _generate_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        summary = {
            'overall_performance': 'PASS',
            'key_metrics': {},
            'recommendations': []
        }

        # åˆ†æå•è¯·æ±‚æ€§èƒ½
        if 'single_request' in results:
            avg_time = results['single_request']['mean_time']
            summary['key_metrics']['avg_response_time'] = f"{avg_time:.3f}s"

            if avg_time > 2:
                summary['overall_performance'] = 'NEEDS_IMPROVEMENT'
                summary['recommendations'].append("å•è¯·æ±‚å“åº”æ—¶é—´è¶…è¿‡2ç§’ï¼Œå»ºè®®ä¼˜åŒ–æ¨¡å‹è°ƒç”¨")

        # åˆ†æç¼“å­˜æ€§èƒ½
        if 'cache' in results:
            hit_rate = results['cache']['cache_hit_rate']
            summary['key_metrics']['cache_hit_rate'] = f"{hit_rate*100:.1f}%"

            if hit_rate < 0.6:
                summary['recommendations'].append("ç¼“å­˜å‘½ä¸­ç‡ä½äº60%ï¼Œå»ºè®®å¢åŠ ç¼“å­˜å®¹é‡æˆ–ä¼˜åŒ–ç¼“å­˜é”®")

        # åˆ†æå¹¶å‘æ€§èƒ½
        if 'concurrent' in results:
            throughput = results['concurrent']['throughput']
            summary['key_metrics']['throughput'] = f"{throughput:.2f} req/s"

            if throughput < 10:
                summary['recommendations'].append("ååé‡ä½äº10 req/sï¼Œå»ºè®®å¢åŠ å¹¶å‘å¤„ç†èƒ½åŠ›")

        return summary

    def _generate_html_report(self, report: Dict[str, Any], output_file: str):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>æ€§èƒ½æµ‹è¯•æŠ¥å‘Š - {report['timestamp']}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
        .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }}
        .summary {{ background: white; padding: 20px; margin: 20px 0; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .metric {{ display: inline-block; margin: 10px 20px; }}
        .metric-value {{ font-size: 24px; font-weight: bold; color: #3498db; }}
        .metric-label {{ color: #7f8c8d; margin-top: 5px; }}
        .test-result {{ background: white; padding: 15px; margin: 10px 0; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .pass {{ color: #27ae60; }}
        .fail {{ color: #e74c3c; }}
        .recommendation {{ background: #fff3cd; padding: 10px; margin: 5px 0; border-left: 4px solid #ffc107; }}
        table {{ width: 100%; border-collapse: collapse; }}
        th, td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background: #ecf0f1; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>AI-PPT-Assistant æ€§èƒ½æµ‹è¯•æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {report['timestamp']}</p>
    </div>

    <div class="summary">
        <h2>æµ‹è¯•æ‘˜è¦</h2>
        <div>
            <div class="metric">
                <div class="metric-value">{report['summary']['key_metrics'].get('avg_response_time', 'N/A')}</div>
                <div class="metric-label">å¹³å‡å“åº”æ—¶é—´</div>
            </div>
            <div class="metric">
                <div class="metric-value">{report['summary']['key_metrics'].get('cache_hit_rate', 'N/A')}</div>
                <div class="metric-label">ç¼“å­˜å‘½ä¸­ç‡</div>
            </div>
            <div class="metric">
                <div class="metric-value">{report['summary']['key_metrics'].get('throughput', 'N/A')}</div>
                <div class="metric-label">ååé‡</div>
            </div>
        </div>
    </div>

    <div class="summary">
        <h2>æµ‹è¯•ç»“æœ</h2>
        {self._format_test_results_html(report['results'])}
    </div>

    <div class="summary">
        <h2>ä¼˜åŒ–å»ºè®®</h2>
        {''.join([f'<div class="recommendation">{r}</div>' for r in report['summary']['recommendations']])}
    </div>
</body>
</html>
"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

    def _format_test_results_html(self, results: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æµ‹è¯•ç»“æœä¸ºHTML"""
        html = ""

        for test_name, test_data in results.items():
            if isinstance(test_data, dict) and 'test_name' in test_data:
                html += f"""
                <div class="test-result">
                    <h3>{test_data['test_name']}</h3>
                    <table>
                """
                for key, value in test_data.items():
                    if key != 'test_name':
                        if isinstance(value, float):
                            value = f"{value:.3f}"
                        html += f"<tr><td>{key}</td><td>{value}</td></tr>"

                html += """
                    </table>
                </div>
                """

        return html

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.service:
            self.service.cleanup()
        if self.monitor:
            self.monitor.shutdown()
        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¿è¡ŒAI-PPT-Assistantæ€§èƒ½æµ‹è¯•')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default=f'performance_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json',
                       help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    parser.add_argument('--use-real-api', action='store_true', help='ä½¿ç”¨çœŸå®APIè€Œéæ¨¡æ‹Ÿ')
    parser.add_argument('--run-stress', action='store_true', help='è¿è¡Œå‹åŠ›æµ‹è¯•')
    parser.add_argument('--iterations', type=int, default=10, help='å•è¯·æ±‚æµ‹è¯•è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--batch-size', type=int, default=10, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--concurrent', type=int, default=20, help='å¹¶å‘è¯·æ±‚æ•°')

    args = parser.parse_args()

    # åŠ è½½æˆ–åˆ›å»ºé…ç½®
    if args.config:
        with open(args.config, 'r') as f:
            config = json.load(f)
    else:
        config = {
            'use_real_api': args.use_real_api,
            'enable_cache': True,
            'enable_metrics': True,
            'enable_monitoring': False,
            'max_workers': 10,
            'single_iterations': args.iterations,
            'batch_size': args.batch_size,
            'concurrent_requests': args.concurrent,
            'run_stress_test': args.run_stress,
            'stress_duration': 30
        }

    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = PerformanceTestRunner(config)

    try:
        # è®¾ç½®ç¯å¢ƒ
        runner.setup()

        # è¿è¡Œæµ‹è¯•
        results = asyncio.run(runner.run_all_tests())

        # ç”ŸæˆæŠ¥å‘Š
        runner.generate_report(results, args.output)

        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 50)
        print("ğŸ“Š æ€§èƒ½æµ‹è¯•æ‘˜è¦")
        print("=" * 50)

        if 'single_request' in results:
            print(f"å¹³å‡å“åº”æ—¶é—´: {results['single_request']['mean_time']:.3f}ç§’")

        if 'cache' in results:
            print(f"ç¼“å­˜åŠ é€Ÿ: {results['cache']['speedup']:.2f}x")
            print(f"ç¼“å­˜å‘½ä¸­ç‡: {results['cache']['cache_hit_rate']*100:.1f}%")

        if 'concurrent' in results:
            print(f"å¹¶å‘ååé‡: {results['concurrent']['throughput']:.2f} req/s")
            print(f"å¹¶å‘æˆåŠŸç‡: {results['concurrent']['success_rate']*100:.1f}%")

    finally:
        # æ¸…ç†èµ„æº
        runner.cleanup()


if __name__ == "__main__":
    main()